2025-05-07 20:14:59.091 | INFO     | agents.umlagent:create_initial_plan:202 - Creating initial plan with ID: plan_1746620099
2025-05-07 20:15:03.644 | INFO     | agents.umlagent:create_initial_plan:226 - Executed tool planning with result:  `å·¥å…·:planning`çš„è§‚æµ‹ç»“æœè¾“å‡ºä¸º :
Plan created successfully with ID: plan_1746620099

Plan: æ‹‰å–GITHUBé¡¹ç›®å¹¶æŸ¥æ‰¾å…³äºattentionçš„éƒ¨åˆ† (ID: plan_1746620099)
========================================================

Progress: 0/3 steps completed (0.0%)
Status: 0 completed, 0 in progress, 0 blocked, 3 not started

Steps:
0. [ ] ç¡®å®šè¦æ‹‰å–çš„GITHUBé¡¹ç›®åç§°ï¼Œè¯¥é¡¹ç›®éœ€ä¸QWENç›¸å…³
1. [ ] ä½¿ç”¨github_repo_clonerå·¥å…·å…‹éš†æ‰¾åˆ°çš„GITHUBé¡¹ç›®åˆ°æœ¬åœ°
2. [ ] åœ¨å…‹éš†çš„é¡¹ç›®ä¸­ä½¿ç”¨file_seekerå·¥å…·æœç´¢å…³äº'attention'çš„éƒ¨åˆ†

2025-05-07 20:15:03.644 | INFO     | agents.base:run:119 - Executing step 1/20
2025-05-07 20:15:06.169 | INFO     | agents.tool_call:think:58 - âœ¨ uml agnet çš„æƒ³æ³•ä¸º: è®¡åˆ’ç›®å‰æ˜¯è¶³å¤Ÿçš„ï¼Œä¸éœ€è¦è¿›ä¸€æ­¥çš„ç»†åŒ–ã€‚æˆ‘ç°åœ¨å¯ä»¥ç«‹å³æ‰§è¡Œä¸‹ä¸€æ­¥ï¼Œå³ç¡®å®šä¸€ä¸ªä¸QWENç›¸å…³çš„GITHUBé¡¹ç›®åç§°ã€‚
2025-05-07 20:15:06.179 | INFO     | agents.tool_call:think:59 - ğŸ› ï¸ uml agnet é€‰æ‹©äº† 1 ä¸ªå·¥å…·
2025-05-07 20:15:06.179 | INFO     | agents.tool_call:think:63 - ğŸ§° é€‰æ‹©çš„å·¥å…·ä¿¡æ¯: ['baidu_search']
2025-05-07 20:15:07.362 | INFO     | agents.tool_call:act:115 - ğŸ¯ å·¥å…· 'baidu_search' å®Œæˆäº†å®ƒçš„ä»»åŠ¡! å…¶æ‰§è¡Œç»“æœä¸º:  `å·¥å…·:baidu_search`çš„è§‚æµ‹ç»“æœè¾“å‡ºä¸º :
[{'title': 'å…¨çƒæœ€å¼ºå¼€æºæ¨¡å‹Qwen3å…¨é¢è§£æ:æ‰‹æœºåˆ°è¶…ç®—,æœ‰æœ›ç™»é¡¶ç›®å‰A...', 'abstract': 'å…¨çƒæœ€å¼ºå¼€æºæ¨¡å‹Qwen3å…¨é¢è§£æ:æ‰‹æœºåˆ°è¶…ç®—,æœ‰æœ›ç™»é¡¶ç›®å‰A...\n\n\n\n\n\n\n\n\n8å¤©å‰ 1. GitHubé¡¹ç›®çš„ç®€ä»‹ é¡¹ç›®åˆ›å»ºè€…:Qwen3ç”±é˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹é€šä¹‰åƒé—®å›¢é˜Ÿ(Qwen Team)å¼€å‘,æ˜¯ç»§QwQã€Qwen2.5ä¹‹åçš„ç¬¬ä¸‰ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ã€‚ åŠŸèƒ½ç‰¹ç‚¹: æ··åˆæ¨ç†æ¶æ„:é¦–åˆ›â€œæ€è€ƒæ¨¡å¼â€...\n\n\n\n\nçŸ¥ä¹\ue62b', 'url': 'http://www.baidu.com/link?url=3FFYf6rdIOUpK2s7QhFHMtEVVXGB24YbkbGwpX6noVxVP2kBz6ADIVE_NDiH2CSfoIJpx0VcKMtZYxLnVl3t8q', 'rank': 1}, {'title': 'é˜¿é‡ŒQwen3 é‡ç£…å¼€æº,å…¨çƒ119è¯­ç§é€šåƒ_æ¾æ¹ƒå·Â·æ¹ƒå®¢_æ¾æ¹ƒæ–°...', 'abstract': 'é˜¿é‡ŒQwen3 é‡ç£…å¼€æº,å…¨çƒ119è¯­ç§é€šåƒ_æ¾æ¹ƒå·Â·æ¹ƒå®¢_æ¾æ¹ƒæ–°...\n\n\n\n\n\n\n\n\n2å¤©å‰ æœ€å¼•äººæ³¨ç›®çš„æ——èˆ°æ¨¡å‹Qwen3-235B-A22B,åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­ç›´æ¥å¯¹æ ‡OpenAIçš„o3-miniå’Œè°·æ­ŒGemini 2.5 Proã€‚ ç›®å‰,ç›®å‰åŒ…æ‹¬32Bç‰ˆæœ¬åœ¨å†…çš„å¤šä¸ªæ¨¡å‹å·²ç»ç™»é™†Hugging Faceå’ŒGitHubã€‚ Webç«¯:ht...æ¾æ¹ƒæ–°é—»\ue62b\n\n\ue680\næ’­æŠ¥\n\n\n\ue67d\næš‚åœ', 'url': 'http://www.baidu.com/link?url=KnCUnM0ajasZnonZov2EBczbArI9SF4J9KgqZk3-pJTaNLU_UCZLRh2gRb4m8pE6kvQVsQJnsM_GVA-pCSxBk_', 'rank': 2}, {'title': 'qwen github organization - Qwen Â· GitHub', 'abstract': 'qwen github organization - Qwen Â· GitHubæŸ¥çœ‹æ­¤ç½‘é¡µçš„ä¸­æ–‡ç¿»è¯‘ï¼Œè¯·ç‚¹å‡»\xa0ç¿»è¯‘æ­¤é¡µ\n\n\n\n\n\n\n\n\n4å¤©å‰ https://chat.qwen.ai/ @Alibaba_Qwen company/qwen https://qwenlm.github.io qianwen_opensource@alibabacloud.com Overview Repositories22 Projects Packages People5 More Pinned...GitHub\ue62b', 'url': 'http://www.baidu.com/link?url=6HjdaxP62jMtf43x3jlSK_rUAsBS8z2e93jqFPihTsvG_XwEAZSdtccVE8lDAssZ', 'rank': 3}, {'title': 'qwen Â· GitHub Topics Â· GitHub', 'abstract': 'qwen Â· GitHub Topics Â· GitHub\n\n\n\n\n\n\n\n\n3å¤©å‰ Add a description, image, and links to the qwen topic page so that developers can more easily learn about it.  Curate this topic  Add this topic to your repo  To associate...GitHub\ue62b', 'url': 'http://www.baidu.com/link?url=xcpR3DMiqtiFAoJ83UAE147nbHyhr4btFSB7uGpRnYlNQ0HqkVk94CpxRcskV2dE', 'rank': 4}, {'title': 'é€šä¹‰åƒé—® Qwen3 éœ‡æ’¼å¼€æº! - å“”å“©å“”å“©', 'abstract': 'é€šä¹‰åƒé—® Qwen3 éœ‡æ’¼å¼€æº! - å“”å“©å“”å“©\n\n\n\n\n\n\n\n\n8å¤©å‰ æ­¤æ¬¡å¼€æºå»¶ç»­äº†é€šä¹‰åƒé—®çš„æŠ€æœ¯ä¼˜åŠ¿,åœ¨ä»£ç ç†è§£ã€å¤šè¯­è¨€æ”¯æŒ(119ç§è¯­è¨€)åŠæ··åˆæ¨ç†æ¨¡å¼ä¸Šå®ç°çªç ´ã€‚æ¨¡å‹å·²åœ¨Hugging Faceã€ModelScopeç­‰å¹³å°å¼€æ”¾å•†ç”¨,æµ‹è¯•å¼€å‘è€…å¯é€šè¿‡APIè°ƒç”¨æˆ–æœ¬åœ°éƒ¨...å“”å“©å“”å“©\ue62b\n\n\ue680\næ’­æŠ¥\n\n\n\ue67d\næš‚åœ', 'url': 'http://www.baidu.com/link?url=6HjdaxP62jMtf43x3jlSKGkDvKwRcwiRoFRaACx_jcmPybhTby3FqP7cezyeYMu8_v2VycnKpM6NdZQA9bXA5q', 'rank': 5}, {'title': 'å¤§å®¶è¿˜åœ¨æœgithubå›½å†…ç‰ˆGithubå®˜ç½‘githubé¦–é¡µå¼€æºç½‘ç«™githubgithub trendingeç«™Githubgithubè®ºå›githubæ±‰åŒ–qnotified githubgithubä¼˜ç§€webå¼€æºé¡¹ç›®', 'abstract': 'å¤§å®¶è¿˜åœ¨æœgithubå›½å†…ç‰ˆGithubå®˜ç½‘githubé¦–é¡µå¼€æºç½‘ç«™githubgithub trendingeç«™Githubgithubè®ºå›githubæ±‰åŒ–qnotified githubgithubä¼˜ç§€webå¼€æºé¡¹ç›®', 'url': '/s?wd=github%E5%9B%BD%E5%86%85%E7%89%88&usm=1&ie=utf-8&rsv_pq=ff2c87420031fefb&oq=QWEN%20github%20project&rsv_t=299a5e392Wms2OVaYrrbC%2BRTGoQMmKlwa6UKmM7AnTAAj0w4o6QWa7C9WE4&rsf=101636803&rsv_dl=0_prs_28608_1', 'rank': 6}, {'title': 'LLMä»£ç å®ç°-Qwen(ä¸‹è½½å’Œè°ƒç”¨)_qwen github-CSDNåšå®¢', 'abstract': 'LLMä»£ç å®ç°-Qwen(ä¸‹è½½å’Œè°ƒç”¨)_qwen github-CSDNåšå®¢\n\n\n\n\n\n\n\n\n2024å¹´9æœˆ4æ—¥  é¦–å…ˆè¿›å…¥ Qwen å®˜æ–¹çš„Githubæ‹‰å–é¡¹ç›®:github.com/QwenLM/Qwenå¹¶å®‰è£…ç›¸åº”ä¾èµ–ã€‚ git clone https://github.com/QwenLM/Qwen.git cd Qwen pip install -r requirements.txt 1 2 ...\n\n\n\n\nCSDNåšå®¢\ue62b\n\n\ue680\næ’­æŠ¥\n\n\n\ue67d\næš‚åœ', 'url': 'http://www.baidu.com/link?url=A3LWTtKqvvPxzvkNrCD0pV0c5hXOh0f_i70DnOWwA0Jfg_1ohMHshARaWb0MXt9JSpnIAMcAsulSANJaad_igIxgZx6gDOOH-3ExBeXNdYm', 'rank': 7}, {'title': 'æ¯å‘¨GitHub æ¢ç´¢|Qwen Coderã€Payload CMS ç­‰ 9 ä¸ªç²¾é€‰å¼€...', 'abstract': 'æ¯å‘¨GitHub æ¢ç´¢|Qwen Coderã€Payload CMS ç­‰ 9 ä¸ªç²¾é€‰å¼€...\n\n\n\n\n\n\n\n\n2024å¹´11æœˆ23æ—¥  1.Qwen2.5-Coderï¼šå¼€æ”¾ã€å¼ºå¤§ã€æœ‰æ½œåŠ›çš„ä»£ç æ¨¡å‹2.Payloadï¼šNext.js æ— å¤´ CMS3.å¼€æºäºŒåæ¡CRMï¼šæ‘†è„±ä¾›åº”å•†é”å®šï¼Œæ‹¥æŠ±å®šåˆ¶åŒ–4.libSQLï¼šSQLite çš„å¼€æºæ´¾ç”Ÿé¡¹ç›®5.æ¡Œé¢ç‰ˆAIåä½œ...\n\n\n\n\nè¯šå“¥çœ‹å¼€æº\ue62b\n\n\ue680\næ’­æŠ¥\n\n\n\ue67d\næš‚åœ', 'url': 'http://www.baidu.com/link?url=ddG5CkH8qDV9sZEQP_FfI-iFt9Lljc12sO3BVP-wrnCIuplutS3VWtUABhBEj4TodaIrMOn1xM26asV4Ms38mff4a87w4FwK0C16-6Po_pO', 'rank': 8}, {'title': 'ä»Šå¤©Githubå“ªäº›é¡¹ç›®ä¸Šæ¦œäº†?ğŸ”¥', 'abstract': 'ä»Šå¤©Githubå“ªäº›é¡¹ç›®ä¸Šæ¦œäº†?ğŸ”¥\n\n\n\n\n\n\n\n\n2025å¹´2æœˆ24æ—¥  Qwen2.5 æ˜¯ç”± Qwen å›¢é˜Ÿå’Œé˜¿é‡Œäº‘å¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—,æ—¨åœ¨å¸®åŠ©å¼€å‘è€…å’Œä¼ä¸šæå‡æ™ºèƒ½åº”ç”¨çš„æ•´ä½“ä½“éªŒã€‚ QwenèŠå¤©åŠ©æ‰‹ ğŸ—¨ï¸ğŸ¤– é˜¿é‡Œäº‘æ¨å‡ºçš„ Qwen æ˜¯ä½ çš„ä¸­æ–‡å¥½ä¼™ä¼´,ä¸“ä¸ºé«˜æ•ˆå¤„...\n\n\n\n\nä¸‡çš„Luccaæ—¶å…‰å·®\ue62b\n\n\ue680\næ’­æŠ¥\n\n\n\ue67d\næš‚åœ', 'url': 'http://www.baidu.com/link?url=A3LWTtKqvvPxzvkNrCD0pXnKUviFIOJNuiTA3RwEe6wi7doi4RdqU6JCSV4A1NhiZQrY0Fx9Rth0u54HO3ElUN2vehEFHm_WezzN9VbOEKf8lPPlcYhRH6eTsrRlmspz', 'rank': 9}, {'title': 'ã€é€šä¹‰åƒé—®ã€‘å¤§æ¨¡å‹Qwen GitHubå¼€æºå·¥ç¨‹å­¦ä¹ ç¬”è®°(1)-- ä½¿...', 'abstract': 'ã€é€šä¹‰åƒé—®ã€‘å¤§æ¨¡å‹Qwen GitHubå¼€æºå·¥ç¨‹å­¦ä¹ ç¬”è®°(1)-- ä½¿...\n\n\n\n\n\n\n\n\n9æœˆ25æ—¥,é˜¿é‡Œäº‘å¼€æºé€šä¹‰åƒé—®140äº¿å‚æ•°æ¨¡å‹Qwen-14BåŠå…¶å¯¹è¯æ¨¡å‹Qwen-14B-Chat,å…è´¹å¯å•†ç”¨ã€‚ ç«‹é©¬å°±åˆ°äº†GitHubå»forkã€‚ GitHub: GitHub - QwenLM/Qwen: The official repo of Qwen (é€šä¹‰åƒé—®...\n\n\n\n\nCSDNä¸‹è½½\ue62b\n\n\ue680\næ’­æŠ¥\n\n\n\ue67d\næš‚åœ', 'url': 'http://www.baidu.com/link?url=6HjdaxP62jMtf43x3jlSKGkDvKwRcwiRoFRaACx_jcoLGSHK1uakhGzoL9QfvaDWfvLkJ_V-Z3CQEwg5dndrzfiVX1r4g8vb3dFKl5O7EtG', 'rank': 10}]
2025-05-07 20:15:07.364 | INFO     | agents.umlagent:update_plan_status:151 - Marked step 0 as completed in plan plan_1746620099
2025-05-07 20:15:07.364 | INFO     | agents.base:run:119 - Executing step 2/20
2025-05-07 20:15:10.893 | INFO     | agents.tool_call:think:58 - âœ¨ uml agnet çš„æƒ³æ³•ä¸º: æˆ‘å·²ç»æ‰¾åˆ°äº†ä¸€ä¸ªä¸QWENç›¸å…³çš„GITHUBé¡¹ç›®åç§°ï¼Œå³'QwenLM/Qwen'ã€‚ç°åœ¨å¯ä»¥æ‰§è¡Œä¸‹ä¸€æ­¥ï¼Œä½¿ç”¨github_repo_clonerå·¥å…·å…‹éš†æ‰¾åˆ°çš„GITHUBé¡¹ç›®åˆ°æœ¬åœ°ã€‚
2025-05-07 20:15:10.893 | INFO     | agents.tool_call:think:59 - ğŸ› ï¸ uml agnet é€‰æ‹©äº† 1 ä¸ªå·¥å…·
2025-05-07 20:15:10.893 | INFO     | agents.tool_call:think:63 - ğŸ§° é€‰æ‹©çš„å·¥å…·ä¿¡æ¯: ['github_repo_cloner']
2025-05-07 20:15:10.893 | INFO     | agents.tool_call:act:115 - ğŸ¯ å·¥å…· 'github_repo_cloner' å®Œæˆäº†å®ƒçš„ä»»åŠ¡! å…¶æ‰§è¡Œç»“æœä¸º:  `å·¥å…·:github_repo_cloner`çš„è§‚æµ‹ç»“æœè¾“å‡ºä¸º :
é”™è¯¯ï¼šç›®æ ‡ç›®å½• 'D:\deep_learning\codes\workspace\Qwen' å·²å­˜åœ¨ä¸”éç©ºï¼Œå¯èƒ½ä»“åº“å·²è¢«å…‹éš†ã€‚è¯·å…ˆæ‰‹åŠ¨å¤„ç†è¯¥ç›®å½•ã€‚
2025-05-07 20:15:10.902 | INFO     | agents.umlagent:update_plan_status:151 - Marked step 1 as completed in plan plan_1746620099
2025-05-07 20:15:10.902 | INFO     | agents.base:run:119 - Executing step 3/20
2025-05-07 20:15:13.576 | INFO     | agents.tool_call:think:58 - âœ¨ uml agnet çš„æƒ³æ³•ä¸º: 
2025-05-07 20:15:13.576 | INFO     | agents.tool_call:think:59 - ğŸ› ï¸ uml agnet é€‰æ‹©äº† 1 ä¸ªå·¥å…·
2025-05-07 20:15:13.576 | INFO     | agents.tool_call:think:63 - ğŸ§° é€‰æ‹©çš„å·¥å…·ä¿¡æ¯: ['file_seeker']
2025-05-07 20:15:26.940 | INFO     | agents.tool_call:act:115 - ğŸ¯ å·¥å…· 'file_seeker' å®Œæˆäº†å®ƒçš„ä»»åŠ¡! å…¶æ‰§è¡Œç»“æœä¸º:  `å·¥å…·:file_seeker`çš„è§‚æµ‹ç»“æœè¾“å‡ºä¸º :
åœ¨è·¯å¾„ 'D:\deep_learning\codes\workspace\Qwen' ä¸­æœç´¢ 'attention' çš„ç»“æœï¼š

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\FAQ.md
  ç¬¬ 5 è¡Œ: #### Failure in installing flash attention
  ç¬¬ 7 è¡Œ: Flash attention is an option for accelerating training and inference. Only NVIDIA GPUs of Turing, Ampere, Ada, and Hopper architecture, e.g., H100, A100, RTX 3090, T4, RTX 2080, can support flash attention. **You can use our models without installing it.**

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\FAQ_ja.md
  ç¬¬ 5 è¡Œ: #### Flash attention å°å…¥ã®å¤±æ•—ä¾‹
  ç¬¬ 7 è¡Œ: Flash attention ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã‚’åŠ é€Ÿã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ã€‚H100ã€A100ã€RTX 3090ã€T4ã€RTX 2080 ãªã©ã® Turingã€Ampereã€Adaã€ãŠã‚ˆã³ Hopper ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã® NVIDIA GPU ã ã‘ãŒã€flash attention ã‚’ã‚µãƒãƒ¼ãƒˆã§ãã¾ã™ã€‚ãã‚Œã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã›ãšã«ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\FAQ_zh.md
  ç¬¬ 5 è¡Œ: #### flash attention å®‰è£…å¤±è´¥
  ç¬¬ 7 è¡Œ: flash attentionæ˜¯ä¸€ä¸ªç”¨äºåŠ é€Ÿæ¨¡å‹è®­ç»ƒæ¨ç†çš„å¯é€‰é¡¹ï¼Œä¸”ä»…é€‚ç”¨äºTuringã€Ampereã€Adaã€Hopperæ¶æ„çš„Nvidia GPUæ˜¾å¡ï¼ˆå¦‚H100ã€A100ã€RTX 3090ã€T4ã€RTX 2080ï¼‰ï¼Œæ‚¨å¯ä»¥åœ¨ä¸å®‰è£…flash attentionçš„æƒ…å†µä¸‹æ­£å¸¸ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\finetune.py
  ç¬¬ 175 è¡Œ: attention_mask=input_ids.ne(tokenizer.pad_token_id),
  ç¬¬ 191 è¡Œ: self.attention_mask = data_dict["attention_mask"]
  ç¬¬ 200 è¡Œ: attention_mask=self.attention_mask[i],
  ç¬¬ 228 è¡Œ: attention_mask=ret["attention_mask"][0],

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\README.md
  ç¬¬ 112 è¡Œ: * CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)
  ç¬¬ 127 è¡Œ: If your device supports fp16 or bf16, we recommend installing [flash-attention](https://github.com/Dao-AILab/flash-attention) (**we support flash attention 2 now.**) for higher efficiency and lower memory usage. (**flash-attention is optional and the project can run normally without installing it**)
  ç¬¬ 130 è¡Œ: git clone https://github.com/Dao-AILab/flash-attention
  ç¬¬ 131 è¡Œ: cd flash-attention && pip install .
  ç¬¬ 270 è¡Œ: Qwen supports batch inference. With flash attention enabled, using batch inference can bring a 40% speedup. The example code is shown below:
  ç¬¬ 277 è¡Œ: # To generate attention masks automatically, it is necessary to assign distinct
  ç¬¬ 469 è¡Œ: The attention KV cache can be quantized and compressed for storage, to get a higher sample throughput. The arguments `use_cache_quantization` and `use_cache_kernel` in `config.json` are provided to enable KV cache quantization. The specific use method is as follows:
  ç¬¬ 480 è¡Œ: Attention: Currently, KV cache quantization and flash attention cannot be used at the same time.
  ç¬¬ 481 è¡Œ: If you enable KV cache quantization and flash attention at the same time (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), `use_flash_attn` is disabled by default (`use_flash_attn=false`).
  ç¬¬ 522 è¡Œ: If you want to use the attention KV which is quantized, you can use the dequantization operation to convert the Int8 key/value back to the float format as follows:
  ç¬¬ 676 è¡Œ: If you still suffer from insufficient memory, you can consider Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs.
  ç¬¬ 796 è¡Œ: We profile the GPU memory and training speed of both LoRA (LoRA (emb) refers to training the embedding and output layer, while LoRA has no trainable embedding and output layer) and Q-LoRA in the setup of single-GPU training. In this test, we experiment on a single A100-SXM4-80G GPU, and we use CUDA 11.8 and Pytorch 2.0. Flash attention 2 is applied. We uniformly use a batch size of 1 and gradient accumulation of 8. We profile the memory (GB) and speed (s/iter) of inputs of different lengths, namely 256, 512, 1024, 2048, 4096, and 8192. We also report the statistics of full-parameter finetuning with Qwen-7B on 2 A100 GPUs. We only report the statistics of 256, 512, and 1024 tokens due to the limitation of GPU memory.
  ç¬¬ 1058 è¡Œ: - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`
  ç¬¬ 1299 è¡Œ: To extend the context length and break the bottleneck of training sequence length, we introduce several techniques, including NTK-aware interpolation, window attention, and LogN attention scaling, to extend the context length of Qwen-14B from 2K to over 8K tokens, and Qwen-1.8B/7B from 8K to 32K tokens.
  ç¬¬ 1364 è¡Œ: Our tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the [documentation](tokenization_note.md).

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\README_CN.md
  ç¬¬ 116 è¡Œ: * å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
  ç¬¬ 131 è¡Œ: å¦‚æœä½ çš„æ˜¾å¡æ”¯æŒfp16æˆ–bf16ç²¾åº¦ï¼Œæˆ‘ä»¬è¿˜æ¨èå®‰è£…[flash-attention](https://github.com/Dao-AILab/flash-attention)ï¼ˆ**å½“å‰å·²æ”¯æŒflash attention 2**ï¼‰æ¥æé«˜ä½ çš„è¿è¡Œæ•ˆç‡ä»¥åŠé™ä½æ˜¾å­˜å ç”¨ã€‚(**flash-attentionåªæ˜¯å¯é€‰é¡¹ï¼Œä¸å®‰è£…ä¹Ÿå¯æ­£å¸¸è¿è¡Œè¯¥é¡¹ç›®**)
  ç¬¬ 134 è¡Œ: git clone https://github.com/Dao-AILab/flash-attention
  ç¬¬ 135 è¡Œ: cd flash-attention && pip install .
  ç¬¬ 267 è¡Œ: åƒé—®æ”¯æŒbatchæ‰¹é‡æ¨ç†ã€‚åœ¨å¼€å¯flash-attentionçš„çŠ¶æ€ä¸‹ï¼Œä½¿ç”¨batchæ¨ç†å¯ä»¥çº¦40%çš„æé€Ÿã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š
  ç¬¬ 479 è¡Œ: æ³¨æ„ï¼šå½“å‰è¯¥åŠŸèƒ½ä¸æ”¯æŒä¸flash attentionåŒæ—¶å¼€å¯ï¼Œå¦‚æœä½ å¼€äº†KV cacheé‡åŒ–çš„åŒæ—¶åˆå¼€äº†flash attentionï¼ˆ`use_flash_attn=True`ï¼Œ `use_cache_quantization=True`, `use_cache_kernel=True`ï¼‰ï¼Œç¨‹åºé»˜è®¤å°†å…³é—­`use_flash_attn`ã€‚
  ç¬¬ 670 è¡Œ: å¦‚æœä½ ä¾ç„¶é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨Q-LoRA ([è®ºæ–‡](https://arxiv.org/abs/2305.14314)) ã€‚è¯¥æ–¹æ³•ä½¿ç”¨4æ¯”ç‰¹é‡åŒ–æ¨¡å‹ä»¥åŠpaged attentionç­‰æŠ€æœ¯å®ç°æ›´å°çš„æ˜¾å­˜å¼€é”€ã€‚
  ç¬¬ 789 è¡Œ: ä¸‹é¢è®°å½•7Bå’Œ14Bæ¨¡å‹åœ¨å•GPUä½¿ç”¨LoRAï¼ˆLoRA (emb)æŒ‡çš„æ˜¯embeddingå’Œè¾“å‡ºå±‚å‚ä¸è®­ç»ƒï¼Œè€ŒLoRAåˆ™ä¸ä¼˜åŒ–è¿™éƒ¨åˆ†å‚æ•°ï¼‰å’ŒQLoRAæ—¶å¤„ç†ä¸åŒé•¿åº¦è¾“å…¥çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„æƒ…å†µã€‚æœ¬æ¬¡è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨CUDA 11.8å’ŒPytorch 2.0ï¼Œå¹¶ä½¿ç”¨äº†flash attention 2ã€‚æˆ‘ä»¬ç»Ÿä¸€ä½¿ç”¨batch sizeä¸º1ï¼Œgradient accumulationä¸º8çš„è®­ç»ƒé…ç½®ï¼Œè®°å½•è¾“å…¥é•¿åº¦åˆ†åˆ«ä¸º256ã€512ã€1024ã€2048ã€4096å’Œ8192çš„æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰å’Œè®­ç»ƒé€Ÿåº¦ï¼ˆs/iterï¼‰ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨2å¼ A100æµ‹äº†Qwen-7Bçš„å…¨å‚æ•°å¾®è°ƒã€‚å—é™äºæ˜¾å­˜å¤§å°ï¼Œæˆ‘ä»¬ä»…æµ‹è¯•äº†256ã€512å’Œ1024tokençš„æ€§èƒ½ã€‚
  ç¬¬ 1053 è¡Œ: - `qwenllm/qwen:cu114`ï¼ˆä¸æ”¯æŒflash-attentionï¼‰ï¼š`>= 470.82.01`

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\README_ES.md
  ç¬¬ 112 è¡Œ: * Se recomienda CUDA 11.4 y superior (esto es para usuarios de GPU, usuarios de flash-attention, etc.)
  ç¬¬ 127 è¡Œ: Si tu dispositivo soporta fp16 o bf16, te recomendamos instalar [flash-attention](https://github.com/Dao-AILab/flash-attention) (**ahora soportamos flash attention 2.**) para una mayor eficiencia y un menor uso de memoria. (**flash-attention es opcional y el proyecto puede ejecutarse normalmente sin instalarlo**)
  ç¬¬ 130 è¡Œ: git clone https://github.com/Dao-AILab/flash-attention
  ç¬¬ 131 è¡Œ: cd flash-attention && pip install .
  ç¬¬ 476 è¡Œ: AtenciÃ³n: Actualmente, la cuantizaciÃ³n de cachÃ© KV y flash attention no se pueden utilizar al mismo tiempo.
  ç¬¬ 477 è¡Œ: Si habilita la cuantizaciÃ³n de cachÃ© KV y flash attention al mismo tiempo (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), `use_flash_attn` estÃ¡ deshabilitado por defecto (`use_flash_attn=false`).
  ç¬¬ 725 è¡Œ: Perfilamos la memoria de la GPU y la velocidad de entrenamiento tanto de LoRA (LoRA (emb) se refiere al entrenamiento de la capa de incrustaciÃ³n y salida, mientras que LoRA no tiene capa de incrustaciÃ³n y salida entrenables) como de Q-LoRA en la configuraciÃ³n de entrenamiento en una sola GPU. En esta prueba, experimentamos con una Ãºnica GPU A100-SXM4-80G, y utilizamos CUDA 11.8 y Pytorch 2.0. Se aplica Flash attention 2. Utilizamos uniformemente un tamaÃ±o de lote de 1 y una acumulaciÃ³n de gradiente de 8. Perfilamos la memoria (GB) y la velocidad (s/iter) de entradas de distintas longitudes, a saber, 256, 512, 1024, 2048, 4096 y 8192. TambiÃ©n presentamos las estadÃ­sticas del ajuste fino de todos los parÃ¡metros con Qwen-7B en 2 GPU A100. SÃ³lo se presentan las estadÃ­sticas de 256, 512 y 1024 tokens debido a la limitaciÃ³n de memoria de la GPU.
  ç¬¬ 918 è¡Œ: - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\README_FR.md
  ç¬¬ 127 è¡Œ: Si votre appareil supporte fp16 ou bf16, nous vous recommandons d'installer [flash-attention](https://github.com/Dao-AILab/flash-attention) (**nous supportons flash-attention 2 maintenant.**) pour une meilleure efficacitÃ© et une moindre utilisation de la mÃ©moire. (**flash-attention est optionnel et le projet peut fonctionner normalement sans l'installer**)
  ç¬¬ 130 è¡Œ: git clone https://github.com/Dao-AILab/flash-attention
  ç¬¬ 131 è¡Œ: cd flash-attention && pip install .
  ç¬¬ 267 è¡Œ: Qwen prend en charge l'infÃ©rence par lots. Lorsque flash attention est activÃ©e, l'utilisation de l'infÃ©rence par lots peut entraÃ®ner une accÃ©lÃ©ration de 40 %. Le code d'exemple est prÃ©sentÃ© ci-dessous:
  ç¬¬ 463 è¡Œ: Le cache KV de l'attention peut Ãªtre quantifiÃ© et compressÃ© pour le stockage, afin d'obtenir un dÃ©bit d'Ã©chantillonnage plus Ã©levÃ©. Les arguments `use_cache_quantization` et `use_cache_kernel` dans `config.json` sont fournis pour activer la quantification du cache KV.
  ç¬¬ 476 è¡Œ: Attention : Actuellement, la quantification du cache KV et flash attention ne peuvent pas Ãªtre utilisÃ©es en mÃªme temps.
  ç¬¬ 477 è¡Œ: Si vous activez la quantification du cache KV et flash attention en mÃªme temps (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), `use_flash_attn` est dÃ©sactivÃ© par dÃ©faut (`use_flash_attn=false`).
  ç¬¬ 518 è¡Œ: Si vous souhaitez utiliser l'attention KV qui est quantifiÃ©e, vous pouvez utiliser l'opÃ©ration de dÃ©quantification pour reconvertir la clÃ©/valeur int8 au format float comme suit
  ç¬¬ 674 è¡Œ: Si vous souffrez toujours d'un manque de mÃ©moire, vous pouvez envisager Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), qui utilise le modÃ¨le de langage quantifiÃ© et d'autres techniques telles que l'attention paginÃ©e pour rÃ©duire encore les coÃ»ts de mÃ©moire.
  ç¬¬ 725 è¡Œ: Nous profilons la mÃ©moire du GPU et la vitesse d'apprentissage de LoRA (LoRA (emb) se rÃ©fÃ¨re Ã  l'apprentissage de la couche d'intÃ©gration et de sortie, tandis que LoRA n'a pas de couche d'intÃ©gration et de sortie pouvant Ãªtre entraÃ®nÃ©e) et de Q-LoRA dans la configuration de l'apprentissage sur un seul GPU. Dans ce test, nous expÃ©rimentons sur un seul GPU A100-SXM4-80G, et nous utilisons CUDA 11.8 et Pytorch 2.0. Flash attention 2 est appliquÃ©. Nous utilisons uniformÃ©ment une taille de lot de 1 et une accumulation de gradient de 8. Nous profilons la mÃ©moire (GB) et la vitesse (s/iter) des entrÃ©es de diffÃ©rentes longueurs, Ã  savoir 256, 512, 1024, 2048, 4096, et 8192. Nous prÃ©sentons Ã©galement les statistiques du rÃ©glage fin de tous les paramÃ¨tres avec Qwen-7B sur 2 GPU A100. Nous ne prÃ©sentons que les statistiques de 256, 512 et 1024 jetons en raison de la limitation de la mÃ©moire du GPU.
  ç¬¬ 920 è¡Œ: - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`
  ç¬¬ 1162 è¡Œ: Pour augmenter la longueur du contexte et Ã©liminer le goulot d'Ã©tranglement que constitue la longueur de la sÃ©quence d'entraÃ®nement, nous introduisons plusieurs techniques, notamment l'interpolation tenant compte des NTK, l'attention par fenÃªtre et la mise Ã  l'Ã©chelle de l'attention LogN, afin d'augmenter la longueur du contexte de Qwen-14B de 2K Ã  plus de 8K tokens, et de Qwen-1.8B/7B de 8K Ã  32K tokens.
  ç¬¬ 1228 è¡Œ: Notre tokenizer basÃ© sur tiktoken est diffÃ©rent des autres tokenizers, par exemple le tokenizer sentencepiece. Vous devez faire attention aux tokens spÃ©ciaux, en particulier lors de la mise au point. Pour des informations plus dÃ©taillÃ©es sur le tokenizer et son utilisation dans le cadre du finetuning, veuillez vous rÃ©fÃ©rer Ã  la [documentation](tokenization_note.md).

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\README_JA.md
  ç¬¬ 128 è¡Œ: ãŠä½¿ã„ã®ãƒ‡ãƒã‚¤ã‚¹ãŒ fp16 ã¾ãŸã¯ bf16 ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å ´åˆã€[flash-attention](https://github.com/Dao-AILab/flash-attention) ï¼ˆflash attention 2ã«å¯¾å¿œã—ã¾ã—ãŸï¼‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šé«˜ã„åŠ¹ç‡ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚(**flash-attention ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã‚ã‚Šã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãªãã¦ã‚‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯æ­£å¸¸ã«å®Ÿè¡Œã§ãã¾ã™**)
  ç¬¬ 131 è¡Œ: git clone https://github.com/Dao-AILab/flash-attention
  ç¬¬ 132 è¡Œ: cd flash-attention && pip install .
  ç¬¬ 728 è¡Œ: ã‚·ãƒ³ã‚°ãƒ«GPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«ãŠã„ã¦ã€LoRA (LoRA(emb)ã¯embeddingã¨å‡ºåŠ›å±¤ã‚’å­¦ç¿’ã•ã›ã‚‹ãŒã€LoRAã¯embeddingã¨å‡ºåŠ›å±¤ã‚’å­¦ç¿’ã•ã›ãªã„) ã¨Q-LoRAã®GPUãƒ¡ãƒ¢ãƒªã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã™ã‚‹ã€‚ã“ã®ãƒ†ã‚¹ãƒˆã§ã¯ã€ã‚·ãƒ³ã‚°ãƒ«A100-SXM4-80G GPUã§å®Ÿé¨“ã—ã€CUDA 11.8ã¨Pytorch 2.0ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚Flash attention 2ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚256ã€512ã€1024ã€2048ã€4096ã€8192ã¨ã„ã†ç•°ãªã‚‹é•·ã•ã®å…¥åŠ›ã®ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰ã¨é€Ÿåº¦ï¼ˆs/iterï¼‰ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚ã¾ãŸã€2å°ã®A100 GPUã‚’ç”¨ã„ãŸQwen-7Bã«ã‚ˆã‚‹ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®çµ±è¨ˆé‡ã‚‚å ±å‘Šã™ã‚‹ã€‚GPUãƒ¡ãƒ¢ãƒªã®åˆ¶é™ã®ãŸã‚ã€256ã€512ã€1024ãƒˆãƒ¼ã‚¯ãƒ³ã®çµ±è¨ˆã®ã¿ã‚’å ±å‘Šã™ã‚‹ã€‚
  ç¬¬ 951 è¡Œ: - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\run_gptq.py
  ç¬¬ 57 è¡Œ: data.append(dict(input_ids=input_id, attention_mask=input_id.ne(tokenizer.pad_token_id)))

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\tech_memo.md
  ç¬¬ 4 è¡Œ: attention.
  ç¬¬ 62 è¡Œ: The following are the main differences from the standard transformer: 1) using untied embedding, 2) using rotary positional embedding, 3) no biases except for QKV in attention, 4) RMSNorm instead of LayerNorm, 5) SwiGLU instead of ReLU, and 6) adopting flash attention to accelerate training.
  ç¬¬ 63 è¡Œ: The model has 32 layers, the embedding dimension is 4096, and the number of attention heads is 32.
  ç¬¬ 185 è¡Œ: We include support for training-free long-context inference based on ntk-aware interpolation, LogN attention scaling, and local window attention.

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\tokenization_note.md
  ç¬¬ 98 è¡Œ: {'input_ids': [1350, 9639, 91, 8691, 723, 427, 91, 82598], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}
  ç¬¬ 105 è¡Œ: {'input_ids': [1350, 9639, 91, 15460, 62, 15, 91, 82598, 151643], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
  ç¬¬ 125 è¡Œ: {'input_ids': [1350, 445, 151643, 899], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}
  ç¬¬ 205 è¡Œ: {'input_ids': [151854], 'token_type_ids': [0], 'attention_mask': [1]}

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\tokenization_note_ja.md
  ç¬¬ 98 è¡Œ: {'input_ids': [1350, 9639, 91, 8691, 723, 427, 91, 82598], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}
  ç¬¬ 105 è¡Œ: {'input_ids': [1350, 9639, 91, 15460, 62, 15, 91, 82598, 151643], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
  ç¬¬ 125 è¡Œ: {'input_ids': [1350, 445, 151643, 899], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\tokenization_note_zh.md
  ç¬¬ 101 è¡Œ: {'input_ids': [1350, 9639, 91, 8691, 723, 427, 91, 82598], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}
  ç¬¬ 108 è¡Œ: {'input_ids': [1350, 9639, 91, 15460, 62, 15, 91, 82598, 151643], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
  ç¬¬ 128 è¡Œ: {'input_ids': [1350, 445, 151643, 899], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}
  ç¬¬ 208 è¡Œ: {'input_ids': [151854], 'token_type_ids': [0], 'attention_mask': [1]}

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\docker\Dockerfile
  ç¬¬ 49 è¡Œ: FROM bundle_req as bundle_flash_attention
  ç¬¬ 55 è¡Œ: test -d flash-attention || git clone -b v2.3.3 https://github.com/Dao-AILab/flash-attention
  ç¬¬ 56 è¡Œ: cd /data/shared/Qwen/flash-attention &&
  ç¬¬ 62 è¡Œ: FROM bundle_flash_attention as bundle_finetune

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\docker\Dockerfile-cu114
  ç¬¬ 49 è¡Œ: FROM bundle_req as bundle_flash_attention
  ç¬¬ 54 è¡Œ: echo "CUDA 11.4 does not support flash-attention, please try other images."
  ç¬¬ 58 è¡Œ: FROM bundle_flash_attention as bundle_finetune

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\docker\Dockerfile-cu121
  ç¬¬ 51 è¡Œ: FROM bundle_req as bundle_flash_attention
  ç¬¬ 57 è¡Œ: test -d flash-attention || git clone -b v2.3.3 https://github.com/Dao-AILab/flash-attention
  ç¬¬ 58 è¡Œ: cd /data/shared/Qwen/flash-attention &&
  ç¬¬ 64 è¡Œ: FROM bundle_flash_attention as bundle_finetune

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\eval\evaluate_ceval.py
  ç¬¬ 71 è¡Œ: attention_mask = input_ids.ne(tokenizer.pad_token_id)
  ç¬¬ 73 è¡Œ: outputs = model(input_ids, attention_mask=attention_mask)["logits"]

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\eval\evaluate_cmmlu.py
  ç¬¬ 77 è¡Œ: attention_mask = input_ids.ne(tokenizer.pad_token_id)
  ç¬¬ 79 è¡Œ: outputs = model(input_ids, attention_mask=attention_mask)["logits"]

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\eval\evaluate_mmlu.py
  ç¬¬ 85 è¡Œ: attention_mask = input_ids.ne(tokenizer.pad_token_id)
  ç¬¬ 87 è¡Œ: outputs = model(input_ids, attention_mask=attention_mask)["logits"]

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\examples\langchain_tooluse.ipynb
  ç¬¬ 314 è¡Œ: "Try importing flash-attention for faster inference...\n",
  ç¬¬ 315 è¡Œ: "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
  ç¬¬ 316 è¡Œ: "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
  ç¬¬ 317 è¡Œ: "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\examples\tokenizer_showcase.ipynb
  ç¬¬ 134 è¡Œ: "File \u001b[1;32mtransformers\\tokenization_utils_base.py:2756\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2746\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2747\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2748\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2749\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2753\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2754\u001b[0m )\n\u001b[1;32m-> 2756\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2757\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2758\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2759\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2760\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2761\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2762\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2763\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2764\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2765\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2766\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2767\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2768\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2769\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2770\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2771\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2772\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2773\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2774\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2775\u001b[0m )\n",
  ç¬¬ 135 è¡Œ: "File \u001b[1;32mtransformers\\tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[0;32m    653\u001b[0m     first_ids,\n\u001b[0;32m    654\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    668\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    669\u001b[0m )\n",
  ç¬¬ 431 è¡Œ: "{'input_ids': [35946, 99639, 91680, 100472], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
  ç¬¬ 500 è¡Œ: "{'input_ids': [151854], 'token_type_ids': [0], 'attention_mask': [1]}"

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\recipes\applications\domain_finetune\qwen_domain_finetune.ipynb
  ç¬¬ 270 è¡Œ: "Try importing flash-attention for faster inference...\n",
  ç¬¬ 271 è¡Œ: "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
  ç¬¬ 370 è¡Œ: "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\recipes\finetune\deepspeed\readme.md
  ç¬¬ 23 è¡Œ: - CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)
  ç¬¬ 157 è¡Œ: Pay attention that the script relies on the model path to identify the model type, so please keep `chat` in the chat model paths.
  ç¬¬ 316 è¡Œ: We profile the GPU memory and training speed of both LoRA (LoRA (emb) refers to training the embedding and output layer, while LoRA has no trainable embedding and output layer) and Q-LoRA in the setup of single-GPU training. In this test, we experiment on a single A100-SXM4-80G GPU, and we use CUDA 11.8 and Pytorch 2.0. Flash attention 2 is applied. We uniformly use a batch size of 1 and gradient accumulation of 8. We profile the memory (GB) and speed (s/iter) of inputs of different lengths, namely 256, 512, 1024, 2048, 4096, and 8192. We also report the statistics of full-parameter fine-tuning with Qwen-7B on 2 A100 GPUs. We only report the statistics of 256, 512, and 1024 tokens due to the limitation of GPU memory.

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\recipes\inference\hf_modelscope\README.md
  ç¬¬ 10 è¡Œ: * CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)
  ç¬¬ 23 è¡Œ: If your device supports fp16 or bf16, we recommend installing [flash-attention](https://github.com/Dao-AILab/flash-attention) (**we support flash attention 2 now.**) for higher efficiency and lower memory usage. (**flash-attention is optional and the project can run normally without installing it**)
  ç¬¬ 26 è¡Œ: git clone https://github.com/Dao-AILab/flash-attention
  ç¬¬ 27 è¡Œ: cd flash-attention && pip install .
  ç¬¬ 166 è¡Œ: Qwen supports batch inference. With flash attention enabled, using batch inference can bring a 40% speedup. The example code is shown below:

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\recipes\inference\quantization\README.md
  ç¬¬ 56 è¡Œ: The attention KV cache can be quantized and compressed for storage, to get a higher sample throughput. The arguments `use_cache_quantization` and `use_cache_kernel` in `config.json` are provided to enable KV cache quantization. The specific use method is as follows:
  ç¬¬ 67 è¡Œ: Attention: Currently, KV cache quantization and flash attention cannot be used at the same time.
  ç¬¬ 68 è¡Œ: If you enable KV cache quantization and flash attention at the same time (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), `use_flash_attn` is disabled by default (`use_flash_attn=false`).
  ç¬¬ 109 è¡Œ: If you want to use the attention KV which is quantized, you can use the dequantization operation to convert the Int8 key/value back to the float format as follows:

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\recipes\inference\tensorrt\README.md
  ç¬¬ 25 è¡Œ: --use_gpt_attention_plugin float16 \

æ–‡ä»¶ï¼šD:\deep_learning\codes\workspace\Qwen\recipes\quickstart\qwen.ipynb
  ç¬¬ 19 è¡Œ: "- CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n",
  ç¬¬ 28 è¡Œ: "If you need to speed up, you can install  `flash-attention`. The details of the installation can be found [here](https://github.com/Dao-AILab/flash-attention)."
  ç¬¬ 37 è¡Œ: "!git clone https://github.com/Dao-AILab/flash-attention\n",
  ç¬¬ 38 è¡Œ: "!cd flash-attention && pip install .\n",
2025-05-07 20:15:26.940 | INFO     | agents.umlagent:update_plan_status:151 - Marked step 2 as completed in plan plan_1746620099
2025-05-07 20:15:26.940 | INFO     | agents.base:run:119 - Executing step 4/20
2025-05-07 20:15:34.201 | INFO     | agents.tool_call:think:58 - âœ¨ uml agnet çš„æƒ³æ³•ä¸º: Using tools
2025-05-07 20:15:34.202 | INFO     | agents.tool_call:think:59 - ğŸ› ï¸ uml agnet é€‰æ‹©äº† 1 ä¸ªå·¥å…·
2025-05-07 20:15:34.202 | INFO     | agents.tool_call:think:63 - ğŸ§° é€‰æ‹©çš„å·¥å…·ä¿¡æ¯: ['terminate']
2025-05-07 20:15:34.202 | INFO     | agents.tool_call:_handle_special_tool:162 - ğŸ Special tool 'terminate' has completed the task!
2025-05-07 20:15:34.202 | INFO     | agents.tool_call:act:115 - ğŸ¯ å·¥å…· 'terminate' å®Œæˆäº†å®ƒçš„ä»»åŠ¡! å…¶æ‰§è¡Œç»“æœä¸º:  `å·¥å…·:terminate`çš„è§‚æµ‹ç»“æœè¾“å‡ºä¸º :
The interaction has been completed with status: success
