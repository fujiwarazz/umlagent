2025-05-15 00:08:43.724 | INFO     | main:websocket_endpoint:36 - WebSocket connection accepted from 127.0.0.1:56017
2025-05-15 00:09:01.049 | INFO     | main:websocket_endpoint:56 - Received message from 127.0.0.1:56017: å¸®æˆ‘æ‰¾ä¸€ä¸ªllama2çš„é¡¹ç›®ï¼Œå¹¶ä¸”åˆ†æuml
2025-05-15 00:09:01.050 | INFO     | agents.umlagent:create_initial_plan:206 - Creating initial plan with ID: plan_1747238923
2025-05-15 00:09:05.517 | INFO     | agents.umlagent:create_initial_plan:231 -  æ‰§è¡Œå·¥å…· planning çš„ç»“æœä¸º:  `å·¥å…·:planning`çš„è§‚æµ‹ç»“æœè¾“å‡ºä¸º :
Plan created successfully with ID: plan_1747238923

Plan: æŸ¥æ‰¾å¹¶åˆ†æä¸€ä¸ªLlama2é¡¹ç›®çš„UMLå›¾ (ID: plan_1747238923)
=================================================

Progress: 0/4 steps completed (0.0%)
Status: 0 completed, 0 in progress, 0 blocked, 4 not started

Steps:
0. [ ] ä½¿ç”¨ç™¾åº¦æœç´¢å·¥å…·æŸ¥æ‰¾Llama2é¡¹ç›®
1. [ ] é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„Llama2é¡¹ç›®è¿›è¡Œå…‹éš†
2. [ ] ç”Ÿæˆæ‰€é€‰Llama2é¡¹ç›®çš„UMLå›¾
3. [ ] åˆ†æå¹¶æä¾›UMLå›¾çš„è§£é‡Š

2025-05-15 00:09:05.518 | INFO     | agents.base:run:119 - websocket initialized finish, state:<starlette.datastructures.State object at 0x105145400>
2025-05-15 00:09:05.518 | INFO     | agents.base:run:126 - Executing step 1/20
2025-05-15 00:09:07.767 | INFO     | agents.tool_call:think:56 - âœ¨ uml agnet çš„æƒ³æ³•ä¸º: è®¡åˆ’å·²åˆ›å»ºï¼Œç°åœ¨éœ€è¦æ‰§è¡Œç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ç™¾åº¦æœç´¢å·¥å…·æŸ¥æ‰¾Llama2é¡¹ç›®ã€‚
2025-05-15 00:09:07.768 | INFO     | agents.tool_call:think:58 - ğŸ› ï¸ uml agnet é€‰æ‹©äº† 1 ä¸ªå·¥å…·
2025-05-15 00:09:07.768 | INFO     | agents.tool_call:think:63 - ğŸ§° é€‰æ‹©çš„å·¥å…·ä¿¡æ¯: ['baidu_search']
2025-05-15 00:09:07.768 | INFO     | agents.tool_call:think:78 - ğŸ§° å·¥å…·çš„å‚æ•°æ˜¯: ['{"query": "llama2 é¡¹ç›® github", "num_results": 5}']
2025-05-15 00:09:08.757 | INFO     | agents.tool_call:act:131 - ğŸ¯ å·¥å…· 'baidu_search' å®Œæˆäº†å®ƒçš„ä»»åŠ¡! å…¶æ‰§è¡Œç»“æœä¸º:  `å·¥å…·:baidu_search`çš„è§‚æµ‹ç»“æœè¾“å‡ºä¸º :
[{'title': 'llama2 Â· GitHub Topics Â· GitHub', 'abstract': 'llama2 Â· GitHub Topics Â· GitHub\n\n\n\n\n\n\n\n\n\n\næŸ¥çœ‹æ­¤ç½‘é¡µçš„ä¸­æ–‡ç¿»è¯‘ï¼Œè¯·ç‚¹å‡»\n\n\nç¿»è¯‘æ­¤é¡µ\n\n\n\n6å¤©å‰ai self-hosted openai llama gpt gpt-4 llm chatgpt llamacpp llama-cpp gpt4all localai llama2 llama-2 code-llama codellama  Updated Apr 23, 2024 TypeScript ymcui / Chinese-LLaMA-Alpaca-2 Star 7.2k  Code Issues Pull requests Discussi', 'url': 'http://www.baidu.com/link?url=TjMRjxyJFL_6SXHk5tLMK97eIZO142snvkK-ncvmLRrjG8Y1TdrGJ82wEEyKP-ov', 'rank': 1}, {'title': 'ã€å¤§æ¨¡å‹ã€‘åŸºäº LlaMA2 çš„é«˜ star çš„ GitHub å¼€æºé¡¹ç›®æ±‡æ€»', 'abstract': 'ã€å¤§æ¨¡å‹ã€‘åŸºäº LlaMA2 çš„é«˜ star çš„ GitHub å¼€æºé¡¹ç›®æ±‡æ€»\n\n\n\n\n\n\n\n\n\n2023å¹´8æœˆ24æ—¥NO4. LinkSoul-AI/Chinese-Llama-2-7b star:1.7K GitHubåœ°å€: https://github.com/LinkSoul-AI/Chinese-Llama-2-7b ä»‹ç»: å¼€æºç¤¾åŒºç¬¬ä¸€ä¸ªèƒ½ä¸‹è½½ã€èƒ½è¿è¡Œçš„ä¸­æ–‡ LLaMA2 æ¨¡å‹! å…¨éƒ¨å¼€æº,å®Œå…¨å¯å•†ç”¨çš„ä¸­æ–‡ç‰ˆ Llama2 æ¨¡å‹åŠä¸­è‹±æ–‡ SFT æ•°æ®é›†,è¾“å…¥æ ¼å¼ä¸¥æ ¼éµå¾ª llama-2-chat æ ¼å¼,å…¼å®¹é€‚é…æ‰€æœ‰é’ˆå¯¹åŸç‰ˆ llama-2...\n\n\nCSDNåšå®¢\n\n\n\n\n\n\n\n\n\ue680\n', 'url': 'http://www.baidu.com/link?url=7czex22P03M7i9TLYL6PfcgnfKoE0AKRMsBOjIYEOC8MicTUvGarDMtQf8ceAlCJZqTXgRjm0Bu2StlP38FFu9pNPgzYFvTWekJYWCcx8L_', 'rank': 2}, {'title': 'GitHub - Rayrtfr/Llama2-Chinese: æœ€å¥½çš„ä¸­æ–‡Llamaå¤§æ¨¡å‹', 'abstract': 'GitHub - Rayrtfr/Llama2-Chinese: æœ€å¥½çš„ä¸­æ–‡Llamaå¤§æ¨¡å‹\n\nMetaå®˜æ–¹åœ¨2023å¹´8æœˆ24æ—¥å‘å¸ƒäº†Code Llama,åŸºäºä»£ç æ•°æ®å¯¹Llama2è¿›è¡Œäº†å¾®è°ƒ,æä¾›ä¸‰ä¸ªä¸åŒåŠŸèƒ½çš„ç‰ˆæœ¬:åŸºç¡€æ¨¡å‹(Code Llama)ã€Pythonä¸“ç”¨æ¨¡å‹(Code Llama - Python)å’ŒæŒ‡ä»¤è·Ÿéšæ¨¡å‹(Code Llama - Instruct),åŒ…å«7Bã€13Bã€34Bä¸‰ç§ä¸åŒå‚æ•°è§„æ¨¡ã€‚ä¸åŒæ¨¡å‹èƒ½åŠ›åŒºåˆ«å¦‚ä¸‹è¡¨æ‰€ç¤º: å…³äºCode Llamaçš„è¯¦ç»†ä¿¡æ¯å¯ä»¥å‚è€ƒå®˜æ–¹Github...\n\n\nGitHub\n\n\n\n\n\n\ue680\næ’­æŠ¥\n\n\n\ue67d\næš‚åœ', 'url': 'http://www.baidu.com/link?url=7czex22P03M7i9TLYL6PfbnhCs1xAtPOkAHckjeuYzRLv5lfGgKH0Ey8mN_lgtG-2Uvaiwi8jVy2V4CdIbgsP_', 'rank': 3}, {'title': 'å¤§å®¶è¿˜åœ¨æœgithubä¼˜ç§€å¼€æºé¡¹ç›®githubé¡¹ç›®lunaç®—æ³•ç¨³å®šå¸githubå®˜ç½‘jennyçŒ«å¨˜lunaæ¨¡ç»„æ–‡ä»¶githubçƒ­é—¨å¼€æºé¡¹ç›®llama2æ¨¡å‹ä¸‹è½½githubé¡¹ç›®åœ°å€githubçƒ­é—¨é¡¹ç›®landengä¸‹è½½github', 'abstract': 'å¤§å®¶è¿˜åœ¨æœgithubä¼˜ç§€å¼€æºé¡¹ç›®githubé¡¹ç›®lunaç®—æ³•ç¨³å®šå¸githubå®˜ç½‘jennyçŒ«å¨˜lunaæ¨¡ç»„æ–‡ä»¶githubçƒ­é—¨å¼€æºé¡¹ç›®llama2æ¨¡å‹ä¸‹è½½githubé¡¹ç›®åœ°å€githubçƒ­é—¨é¡¹ç›®landengä¸‹è½½github', 'url': '/s?wd=github%E4%BC%98%E7%A7%80%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE&usm=4&ie=utf-8&rsv_pq=bc83c2940073b49f&oq=llama2%20%E9%A1%B9%E7%9B%AE%20github&rsv_t=9ef6xbXEbMLLhajNrYJ4O4FoTZ6Xu5iTYt4YUx6TRQka6COKxLAGaP%2Fo1xY&rsf=101633403&rsv_dl=0_prs_28608_1', 'rank': 4}, {'title': 'llama 2 githubå®˜æ–¹æ–‡æ¡£_llama2æ–‡æ¡£-CSDNåšå®¢', 'abstract': 'llama 2 githubå®˜æ–¹æ–‡æ¡£_llama2æ–‡æ¡£-CSDNåšå®¢\n\n2023å¹´7æœˆ29æ—¥Llama2æ˜¯MetaAIå‘å¸ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹,æä¾›7Båˆ°70Bå‚æ•°çš„ç‰ˆæœ¬ã€‚ç”¨æˆ·å¯ä»¥ä¸‹è½½æ¨¡å‹æƒé‡å’Œä»£ç è¿›è¡Œå®éªŒå’Œåˆ›æ–°ã€‚ä¸‹è½½è¿‡ç¨‹æ¶‰åŠæ¥å—è®¸å¯è¯å¹¶é€šè¿‡ç­¾åURLã€‚æ¨¡å‹æ”¯æŒHuggingFaceå¹³å°,éœ€è¦é€‚å½“çš„Pythonç¯å¢ƒå’Œä¾èµ–ã€‚é¢„è®­ç»ƒæ¨¡å‹å’Œå¾®è°ƒåçš„èŠå¤©æ¨¡å‹æœ‰ä¸åŒçš„ä½¿ç”¨æ–¹æ³•ã€‚åŒæ—¶,æ–‡æ¡£å¼ºè°ƒäº†è´Ÿè´£ä»»çš„ä½¿ç”¨å’Œé£é™©æç¤º,æä¾›äº†é”™è¯¯æŠ¥å‘Šå’Œè®¸å¯è¯ä¿¡æ¯ã€‚ \n\n\nCSDNåšå®¢', 'url': 'http://www.baidu.com/link?url=GWD8EuLBtUc673JOHOy5TBIqqM6fwyr5QW63hl4FCaOh2Gy4FzYKDQZlIpzxQUPuKelfhaYuIv2fs2M2W3P_R2Heu8N4DDR5jH7f6Wsqgr3', 'rank': 5}]
2025-05-15 00:09:08.758 | INFO     | agents.umlagent:update_plan_status:155 - Marked step 0 as completed in plan plan_1747238923
2025-05-15 00:09:08.758 | INFO     | agents.base:run:126 - Executing step 2/20
2025-05-15 00:09:14.107 | INFO     | agents.tool_call:think:56 - âœ¨ uml agnet çš„æƒ³æ³•ä¸º: Using tools response: æˆ‘å·²ç»æ‰¾åˆ°äº†ä¸€äº›å…³äºLlama2é¡¹ç›®çš„é“¾æ¥ã€‚ç°åœ¨éœ€è¦é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„é¡¹ç›®è¿›è¡Œå…‹éš†ã€‚æˆ‘å°†é€‰æ‹©ç¬¬ä¸€ä¸ªæœç´¢ç»“æœä¸­çš„é¡¹ç›®ï¼š'ymcui / Chinese-LLaMA-Alpaca-2'ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘å°†æ‰§è¡Œç¬¬äºŒæ­¥ï¼šå…‹éš†è¿™ä¸ªé¡¹ç›®ã€‚
2025-05-15 00:09:14.108 | INFO     | agents.tool_call:think:58 - ğŸ› ï¸ uml agnet é€‰æ‹©äº† 1 ä¸ªå·¥å…·
2025-05-15 00:09:14.108 | INFO     | agents.tool_call:think:63 - ğŸ§° é€‰æ‹©çš„å·¥å…·ä¿¡æ¯: ['github_repo_cloner_ssh']
2025-05-15 00:09:14.108 | INFO     | agents.tool_call:think:78 - ğŸ§° å·¥å…·çš„å‚æ•°æ˜¯: ['{"repo_name": "ymcui/Chinese-LLaMA-Alpaca-2"}']
2025-05-15 00:09:24.172 | INFO     | agents.tool_call:act:131 - ğŸ¯ å·¥å…· 'github_repo_cloner_ssh' å®Œæˆäº†å®ƒçš„ä»»åŠ¡! å…¶æ‰§è¡Œç»“æœä¸º:  `å·¥å…·:github_repo_cloner_ssh`çš„è§‚æµ‹ç»“æœè¾“å‡ºä¸º :
/Users/peelsannaw/Desktop/codes/projects/umlagent/umlagent/workspace/Chinese-LLaMA-Alpaca-2
2025-05-15 00:09:24.172 | INFO     | agents.umlagent:update_plan_status:155 - Marked step 1 as completed in plan plan_1747238923
2025-05-15 00:09:24.172 | INFO     | agents.base:run:126 - Executing step 3/20
2025-05-15 00:09:30.285 | INFO     | agents.tool_call:think:56 - âœ¨ uml agnet çš„æƒ³æ³•ä¸º: Using tools response: å·²æˆåŠŸå…‹éš†Llama2é¡¹ç›®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘å°†æ‰§è¡Œç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ‰€é€‰Llama2é¡¹ç›®çš„UMLå›¾ã€‚
2025-05-15 00:09:30.286 | INFO     | agents.tool_call:think:58 - ğŸ› ï¸ uml agnet é€‰æ‹©äº† 1 ä¸ªå·¥å…·
2025-05-15 00:09:30.286 | INFO     | agents.tool_call:think:63 - ğŸ§° é€‰æ‹©çš„å·¥å…·ä¿¡æ¯: ['code_to_uml_generator_multilang']
2025-05-15 00:09:30.286 | INFO     | agents.tool_call:think:78 - ğŸ§° å·¥å…·çš„å‚æ•°æ˜¯: ['{"repo_path": "/Users/peelsannaw/Desktop/codes/projects/umlagent/umlagent/workspace/Chinese-LLaMA-Alpaca-2", "output_png_path": "/Users/peelsannaw/Desktop/codes/projects/umlagent/umlagent/output/llama2_project_uml.png"}']
2025-05-15 00:09:31.236 | INFO     | tools.uml:execute:696 - å‡†å¤‡é€šè¿‡ WebSocket å‘é€ UML å›¾ç‰‡ 'llama2_project_uml.png'ã€‚
2025-05-15 00:09:31.257 | INFO     | tools.uml:execute:711 - UML å›¾ç‰‡ 'llama2_project_uml.png' å·²é€šè¿‡ WebSocket å‘é€ (891200 å­—èŠ‚)ã€‚
2025-05-15 00:09:31.258 | INFO     | agents.tool_call:act:131 - ğŸ¯ å·¥å…· 'code_to_uml_generator_multilang' å®Œæˆäº†å®ƒçš„ä»»åŠ¡! å…¶æ‰§è¡Œç»“æœä¸º:  `å·¥å…·:code_to_uml_generator_multilang`çš„è§‚æµ‹ç»“æœè¾“å‡ºä¸º :
UML å›¾è¡¨å’Œæ–‡æœ¬æè¿°å·²æˆåŠŸç”Ÿæˆã€‚
PNG å›¾åƒä¿å­˜è·¯å¾„: '/Users/peelsannaw/Desktop/codes/projects/umlagent/umlagent/output/llama2_project_uml.png'

---
UML å›¾è¡¨æ–‡æœ¬æè¿°å†…å®¹:
---
# UML å›¾è¡¨æ–‡æœ¬æè¿°

## Class: `ChatCompletionRequest`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`, `BaseModel`

### å±æ€§ (Attributes)
- `best_of: Optional[int]`
- `do_sample: Optional[bool]`
- `frequency_penalty: Optional[float]`
- `ignore_eos: Optional[bool]`
- `logit_bias: Optional[Dict[str, float]]`
- `max_tokens: Optional[int]`
- `messages: Union[str, List[Dict[str, str]]]`
- `model: str`
- `n: Optional[int]`
- `num_beams: Optional[int]`
- `presence_penalty: Optional[float]`
- `repetition_penalty: Optional[float]`
- `stop: Optional[Union[str, List[str]]]`
- `stream: Optional[bool]`
- `temperature: Optional[float]`
- `top_k: Optional[int]`
- `top_p: Optional[float]`
- `use_beam_search: Optional[bool]`
- `user: Optional[str]`

---

## Class: `ChatCompletionResponse`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`, `BaseModel`

### å±æ€§ (Attributes)
- `choices: List[ChatCompletionResponseChoice]`
- `choices: List[Union[ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice]]`
- `created: int`
- `id: str`
- `model: str`
- `object: str`
- `usage: UsageInfo`

---

## Class: `ChatCompletionResponseChoice`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`, `BaseModel`

### å±æ€§ (Attributes)
- `finish_reason: Optional[Literal['stop', 'length']]`
- `index: int`
- `message: ChatMessage`

---

## Class: `ChatCompletionResponseStreamChoice`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`, `BaseModel`

### å±æ€§ (Attributes)
- `delta: DeltaMessage`
- `finish_reason: Optional[Literal['stop', 'length']]`
- `index: int`

---

## Class: `ChatCompletionStreamResponse`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`

### å±æ€§ (Attributes)
- `choices: List[ChatCompletionResponseStreamChoice]`
- `created: int`
- `id: str`
- `model: str`
- `object: str`

---

## Class: `ChatMessage`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`, `BaseModel`

### å±æ€§ (Attributes)
- `content: str`
- `role: str`

---

## Class: `ChineseLLaMAAlpaca2Adapter`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_server_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModelAdapter`

### æ–¹æ³• (Methods)
- `get_default_conv_template(self, model_path: str) -> Conversation`
- `load_model(self, model_path: str, from_pretrained_kwargs: dict)`
- `match(self, model_path: str)`

---

## Class: `ChineseLLaMAAlpacaAdapter`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_server_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModelAdapter`

### å±æ€§ (Attributes)
- `use_fast_tokenizer`

### æ–¹æ³• (Methods)
- `get_default_conv_template(self, model_path: str) -> Conversation`
- `match(self, model_path: str)`

---

## Class: `CompletionRequest`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`, `BaseModel`

### å±æ€§ (Attributes)
- `best_of: Optional[int]`
- `do_sample: Optional[bool]`
- `echo: Optional[bool]`
- `frequency_penalty: Optional[float]`
- `ignore_eos: Optional[bool]`
- `logit_bias: Optional[Dict[str, float]]`
- `logprobs: Optional[int]`
- `max_tokens: Optional[int]`
- `model: str`
- `n: Optional[int]`
- `num_beams: Optional[int]`
- `presence_penalty: Optional[float]`
- `prompt: Union[str, List[Any]]`
- `prompt: Union[str, List[str]]`
- `repetition_penalty: Optional[float]`
- `stop: Optional[Union[str, List[str]]]`
- `stream: Optional[bool]`
- `suffix: Optional[str]`
- `temperature: Optional[float]`
- `top_k: Optional[int]`
- `top_p: Optional[float]`
- `use_beam_search: Optional[bool]`
- `user: Optional[str]`

---

## Class: `CompletionResponse`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`, `BaseModel`

### å±æ€§ (Attributes)
- `choices: List[CompletionResponseChoice]`
- `created: Optional[int]`
- `created: int`
- `id: Optional[str]`
- `id: str`
- `model: Optional[str]`
- `model: str`
- `object: Optional[str]`
- `object: str`
- `usage: UsageInfo`

---

## Class: `CompletionResponseChoice`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`, `BaseModel`

### å±æ€§ (Attributes)
- `finish_reason: Optional[Literal['stop', 'length']]`
- `index: int`
- `logprobs: Optional[LogProbs]`
- `text: str`

---

## Class: `CompletionResponseStreamChoice`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`

### å±æ€§ (Attributes)
- `finish_reason: Optional[Literal['stop', 'length']]`
- `index: int`
- `logprobs: Optional[LogProbs]`
- `text: str`

---

## Class: `CompletionStreamResponse`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`

### å±æ€§ (Attributes)
- `choices: List[CompletionResponseStreamChoice]`
- `created: int`
- `id: str`
- `model: str`
- `object: str`

---

## Class: `DataCollatorForSupervisedDataset`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `build_dataset`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `object`

### å±æ€§ (Attributes)
- `tokenizer: transformers.PreTrainedTokenizer`

### æ–¹æ³• (Methods)
- `__call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]`

---

## Class: `DataTrainingArguments`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `run_clm_pt_with_peft`

### å±æ€§ (Attributes)
- `block_size: Optional[int]`
- `data_cache_dir: Optional[str]`
- `dataset_config_name: Optional[str]`
- `dataset_dir: Optional[str]`
- `keep_linebreaks: bool`
- `max_eval_samples: Optional[int]`
- `max_seq_length: Optional[int]`
- `max_train_samples: Optional[int]`
- `overwrite_cache: bool`
- `preprocessing_num_workers: Optional[int]`
- `streaming: bool`
- `train_file: Optional[str]`
- `validation_file: Optional[str]`
- `validation_split_percentage: Optional[float]`

### æ–¹æ³• (Methods)
- `__post_init__(self)`

---

## Class: `DeltaMessage`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`, `BaseModel`

### å±æ€§ (Attributes)
- `content: Optional[str]`
- `role: Optional[Literal['user', 'assistant', 'system']]`
- `role: Optional[str]`

---

## Class: `EmbeddingsRequest`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`

### å±æ€§ (Attributes)
- `input: Union[str, List[Any]]`
- `user: Optional[str]`

---

## Class: `EmbeddingsResponse`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`

### å±æ€§ (Attributes)
- `data: List[Dict[str, Any]]`
- `model: str`
- `object: str`

---

## Class: `ErrorResponse`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`

### å±æ€§ (Attributes)
- `code: Optional[str]`
- `message: str`
- `object: str`
- `param: Optional[str]`
- `type: str`

---

## Class: `Evaluator`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `evaluator`

### å±æ€§ (Attributes)
- `choices`
- `k`
- `model_name`
- `model_path`
- `puncs`

### æ–¹æ³• (Methods)
- `__init__(self, choices, model_name, k)`
- `__init__(self, choices, model_path, k)`
- `eval_subject(self, subject_name, test_df, dev_df, few_shot, save_result_dir)`
- `exact_match(self, pred, target)`
- `format_example(self, line, include_answer)`
- `generate_few_shot_prompt(self, subject, dev_df)`
- `normalize_answer(self, s)`

---

## Class: `Iteratorize`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `gradio_demo`

### å±æ€§ (Attributes)
- `c_callback`
- `kwargs`
- `mfunc`
- `q`
- `sentinel`
- `stop_now`
- `thread`

### æ–¹æ³• (Methods)
- `__del__(self)`
- `__enter__(self)`
- `__exit__(self, exc_type, exc_val, exc_tb)`
- `__init__(self, func, kwargs, callback)`
- `__iter__(self)`
- `__next__(self)`

---

## Class: `Linear`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `lora`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `nn.Linear`, `LoraLayer`

### å±æ€§ (Attributes)
- `fan_in_fan_out`

### æ–¹æ³• (Methods)
- `__init__(self, in_features: int, out_features: int, r: int, lora_alpha: int, lora_dropout: float, fan_in_fan_out: bool, merge_weights: bool, **kwargs)`
- `eval(self)`
- `forward(self, x: torch.Tensor)`
- `reset_parameters(self)`
- `train(self, mode: bool)`

---

## Class: `Linear4bit`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `lora`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `bnb.nn.Linear4bit`, `LoraLayer`

### æ–¹æ³• (Methods)
- `__init__(self, in_features, out_features, r: int, lora_alpha: int, lora_dropout: float, **kwargs)`
- `forward(self, x: torch.Tensor)`
- `reset_parameters(self)`

---

## Class: `Linear8bitLt`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `lora`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `bnb.nn.Linear8bitLt`, `LoraLayer`

### æ–¹æ³• (Methods)
- `__init__(self, in_features, out_features, r: int, lora_alpha: int, lora_dropout: float, **kwargs)`
- `forward(self, x: torch.Tensor)`
- `reset_parameters(self)`

---

## Class: `Llama_Evaluator`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `llama2_evaluator`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `Evaluator`, `Evaluator`

### å±æ€§ (Attributes)
- `A_id`
- `B_id`
- `C_id`
- `D_id`
- `device`
- `generation_config`
- `model`
- `model_path`
- `sA_id`
- `sB_id`
- `sC_id`
- `sD_id`
- `tokenizer`
- `verbose`

### æ–¹æ³• (Methods)
- `__init__(self, choices, k, model_path, device, temperature, verbose)`
- `eval_subject(self, subject_name, test_df, dev_df, few_shot, cot, save_result_dir, with_prompt, constrained_decoding, do_test)`
- `extract_answer(self, line, gen_ans)`
- `format_example(self, line, include_answer, cot, with_prompt)`
- `generate_alpaca2_few_shot_prompt(self, subject, dev_df, cot)`
- `generate_few_shot_noprompt(self, subject, dev_df, cot)`
- `generate_few_shot_prompt(self, subject, dev_df, cot)`
- `generate_llama2_few_shot_prompt(self, subject, dev_df, cot)`

---

## Class: `LogProbs`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`

### å±æ€§ (Attributes)
- `text_offset: List[int]`
- `token_logprobs: List[Optional[float]]`
- `tokens: List[str]`
- `top_logprobs: List[Optional[Dict[str, float]]]`

---

## Class: `LoraConfig`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `lora`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PeftConfig`

### å±æ€§ (Attributes)
- `bias: str`
- `enable_lora: Optional[List[bool]]`
- `fan_in_fan_out: bool`
- `lora_alpha: int`
- `lora_dropout: float`
- `merge_weights: bool`
- `modules_to_save: Optional[List[str]]`
- `r: int`
- `target_modules: Optional[Union[List[str], str]]`

### æ–¹æ³• (Methods)
- `__post_init__(self)`

---

## Class: `LoraLayer`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `lora`

### å±æ€§ (Attributes)
- `disable_adapters`
- `lora_alpha`
- `merge_weights`
- `merged`
- `r`

### æ–¹æ³• (Methods)
- `__init__(self, r: int, lora_alpha: int, lora_dropout: float, merge_weights: bool)`

---

## Class: `LoraModel`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `lora`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `torch.nn.Module`

### å±æ€§ (Attributes)
- `forward`
- `model`
- `peft_config`

### æ–¹æ³• (Methods)
- `__getattr__(self, name: str)`
- `__init__(self, config, model)`
- `_find_and_replace(self)`
- `_get_submodules(self, key)`
- `_replace_module(self, parent_module, child_name, new_module, old_module)`
- `_set_adapter_layers(self, enabled)`
- `disable_adapter_layers(self)`
- `enable_adapter_layers(self)`
- `get_peft_config_as_dict(self, inference: bool)`
- `modules_to_save(self)`

---

## Class: `MergedLinear`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `lora`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `nn.Linear`, `LoraLayer`

### å±æ€§ (Attributes)
- `enable_lora`
- `fan_in_fan_out`

### æ–¹æ³• (Methods)
- `__init__(self, in_features: int, out_features: int, r: int, lora_alpha: int, lora_dropout: float, enable_lora: List[bool], fan_in_fan_out: bool, merge_weights: bool, **kwargs)`
- `eval(self)`
- `forward(self, x: torch.Tensor)`
- `reset_parameters(self)`
- `train(self, mode: bool)`
- `zero_pad(self, x)`

---

## Class: `MergedLinear4bit`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `lora`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `bnb.nn.Linear4bit`, `LoraLayer`

### å±æ€§ (Attributes)
- `enable_lora`

### æ–¹æ³• (Methods)
- `__init__(self, in_features: int, out_features: int, r: int, lora_alpha: int, lora_dropout: float, enable_lora: List[bool], **kwargs)`
- `forward(self, x: torch.Tensor)`
- `reset_parameters(self)`
- `zero_pad(self, x)`

---

## Class: `MergedLinear8bitLt`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `lora`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `bnb.nn.Linear8bitLt`, `LoraLayer`

### å±æ€§ (Attributes)
- `enable_lora`

### æ–¹æ³• (Methods)
- `__init__(self, in_features: int, out_features: int, r: int, lora_alpha: int, lora_dropout: float, enable_lora: List[bool], **kwargs)`
- `forward(self, x: torch.Tensor)`
- `reset_parameters(self)`
- `zero_pad(self, x)`

---

## Class: `ModelArguments`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `run_clm_pt_with_peft`

### å±æ€§ (Attributes)
- `cache_dir: Optional[str]`
- `config_name: Optional[str]`
- `config_overrides: Optional[str]`
- `model_name_or_path: Optional[str]`
- `model_revision: str`
- `model_type: Optional[str]`
- `tokenizer_name: Optional[str]`
- `tokenizer_name_or_path: Optional[str]`
- `torch_dtype: Optional[str]`
- `use_auth_token: bool`
- `use_fast_tokenizer: bool`

### æ–¹æ³• (Methods)
- `__post_init__(self)`

---

## Class: `ModelCard`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`

### å±æ€§ (Attributes)
- `created: int`
- `id: str`
- `object: str`
- `owned_by: str`
- `parent: Optional[str]`
- `permission: List[ModelPermission]`
- `root: Optional[str]`

---

## Class: `ModelList`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`

### å±æ€§ (Attributes)
- `data: List[ModelCard]`
- `object: str`

---

## Class: `ModelPermission`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`

### å±æ€§ (Attributes)
- `allow_create_engine: bool`
- `allow_fine_tuning: bool`
- `allow_logprobs: bool`
- `allow_sampling: bool`
- `allow_search_indices: bool`
- `allow_view: bool`
- `created: int`
- `group: Optional[str]`
- `id: str`
- `is_blocking: str`
- `object: str`
- `organization: str`

---

## Class: `MyTrainingArguments`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `run_clm_pt_with_peft`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `TrainingArguments`, `TrainingArguments`

### å±æ€§ (Attributes)
- `debug_mode: Optional[bool]`
- `double_quant: Optional[bool]`
- `full_finetuning: Optional[bool]`
- `load_in_kbits: Optional[int]`
- `lora_alpha: Optional[float]`
- `lora_dropout: Optional[float]`
- `lora_rank: Optional[int]`
- `modules_to_save: Optional[str]`
- `peft_path: Optional[str]`
- `quant_type: Optional[str]`
- `trainable: Optional[str]`
- `use_flash_attention_2: Optional[bool]`

---

## Class: `PeftConfig`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `config`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PeftConfigMixin`

### å±æ€§ (Attributes)
- `base_model_name_or_path: str`
- `inference_mode: bool`
- `peft_type: Union[str, PeftType]`
- `task_type: Union[str, TaskType]`

---

## Class: `PeftConfigMixin`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `config`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PushToHubMixin`

### å±æ€§ (Attributes)
- `peft_type: Optional[PeftType]`

### æ–¹æ³• (Methods)
- `__dict__(self)`
- `from_json_file(cls, path_json_file, **kwargs)`
- `from_pretrained(cls, pretrained_model_name_or_path, **kwargs)`
- `save_pretrained(self, save_directory, **kwargs)`
- `to_dict(self)`

---

## Class: `PeftModel`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `peft_model`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PushToHubMixin`, `torch.nn.Module`

### å±æ€§ (Attributes)
- `base_model`
- `config`
- `device`
- `modules_to_save`
- `peft_config`

### æ–¹æ³• (Methods)
- `__getattr__(self, name: str)`
- `__init__(self, model, peft_config: PeftConfig)`
- `_setup_prompt_encoder(self)`
- `disable_adapter(self)`
- `forward(self, *args, **kwargs)`
- `from_pretrained(cls, model, model_id, **kwargs)`
- `get_base_model(self)`
- `get_prompt(self, batch_size)`
- `get_prompt_embedding_to_save(self)`
- `print_trainable_parameters(self)`
- `save_pretrained(self, save_directory, **kwargs)`

---

## Class: `PeftModelForCausalLM`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `peft_model`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PeftModel`

### å±æ€§ (Attributes)
- `base_model_prepare_inputs_for_generation`

### æ–¹æ³• (Methods)
- `__init__(self, model, peft_config: PeftConfig)`
- `forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)`
- `generate(self, **kwargs)`
- `prepare_inputs_for_generation(self, *args, **kwargs)`

---

## Class: `PeftModelForSeq2SeqLM`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `peft_model`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PeftModel`

### å±æ€§ (Attributes)
- `base_model_prepare_encoder_decoder_kwargs_for_generation`
- `base_model_prepare_inputs_for_generation`

### æ–¹æ³• (Methods)
- `__init__(self, model, peft_config: PeftConfig)`
- `forward(self, input_ids, attention_mask, inputs_embeds, decoder_input_ids, decoder_attention_mask, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)`
- `generate(self, **kwargs)`
- `prepare_inputs_for_generation(self, *args, **kwargs)`

---

## Class: `PeftModelForSequenceClassification`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `peft_model`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PeftModel`

### å±æ€§ (Attributes)
- `modules_to_save`

### æ–¹æ³• (Methods)
- `__init__(self, model, peft_config: PeftConfig)`
- `_prefix_tuning_forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)`
- `forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)`

---

## Class: `PeftModelForTokenClassification`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `peft_model`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PeftModel`

### å±æ€§ (Attributes)
- `modules_to_save`

### æ–¹æ³• (Methods)
- `__init__(self, model, peft_config: PeftConfig)`
- `_prefix_tuning_forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)`
- `forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)`

---

## Class: `PeftType`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `config`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `str`, `enum.Enum`

### å±æ€§ (Attributes)
- `LORA`
- `PREFIX_TUNING`
- `PROMPT_TUNING`
- `P_TUNING`

---

## Class: `PrefixEncoder`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `prefix_tuning`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `torch.nn.Module`

### å±æ€§ (Attributes)
- `prefix_projection`

### æ–¹æ³• (Methods)
- `__init__(self, config)`
- `forward(self, prefix: torch.Tensor)`

---

## Class: `PrefixTuningConfig`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `prefix_tuning`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PromptLearningConfig`

### å±æ€§ (Attributes)
- `encoder_hidden_size: int`
- `prefix_projection: bool`

### æ–¹æ³• (Methods)
- `__post_init__(self)`

---

## Class: `PromptEmbedding`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `prompt_tuning`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `torch.nn.Module`

### å±æ€§ (Attributes)
- `embedding`

### æ–¹æ³• (Methods)
- `__init__(self, config, word_embeddings)`
- `forward(self, indices)`

---

## Class: `PromptEncoder`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `p_tuning`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `torch.nn.Module`

### å±æ€§ (Attributes)
- `embedding`
- `encoder_type`
- `hidden_size`
- `input_size`
- `output_size`
- `token_dim`
- `total_virtual_tokens`

### æ–¹æ³• (Methods)
- `__init__(self, config)`
- `forward(self, indices)`

---

## Class: `PromptEncoderConfig`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `p_tuning`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PromptLearningConfig`

### å±æ€§ (Attributes)
- `encoder_dropout: float`
- `encoder_hidden_size: int`
- `encoder_num_layers: int`
- `encoder_reparameterization_type: Union[str, PromptEncoderReparameterizationType]`

### æ–¹æ³• (Methods)
- `__post_init__(self)`

---

## Class: `PromptEncoderReparameterizationType`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `p_tuning`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `str`, `enum.Enum`

### å±æ€§ (Attributes)
- `LSTM`
- `MLP`

---

## Class: `PromptLearningConfig`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `config`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PeftConfig`

### å±æ€§ (Attributes)
- `num_attention_heads: Optional[int]`
- `num_layers: Optional[int]`
- `num_transformer_submodules: Optional[int]`
- `num_virtual_tokens: int`
- `token_dim: int`

---

## Class: `PromptTuningConfig`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `prompt_tuning`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `PromptLearningConfig`

### å±æ€§ (Attributes)
- `prompt_tuning_init: Union[PromptTuningInit, str]`
- `prompt_tuning_init_text: Optional[str]`
- `tokenizer_name_or_path: Optional[str]`

### æ–¹æ³• (Methods)
- `__post_init__(self)`

---

## Class: `PromptTuningInit`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `prompt_tuning`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `str`, `enum.Enum`

### å±æ€§ (Attributes)
- `RANDOM`
- `TEXT`

---

## Class: `SavePeftModelCallback`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `run_clm_pt_with_peft`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `transformers.TrainerCallback`, `transformers.TrainerCallback`

### æ–¹æ³• (Methods)
- `on_save(self, args, state, control, **kwargs)`
- `on_train_end(self, args, state, control, **kwargs)`
- `save_model(self, args, state, kwargs)`

---

## Class: `Stream`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `gradio_demo`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `StoppingCriteria`

### å±æ€§ (Attributes)
- `callback_func`

### æ–¹æ³• (Methods)
- `__call__(self, input_ids, scores) -> bool`
- `__init__(self, callback_func)`

---

## Class: `TaskType`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `config`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `str`, `enum.Enum`

### å±æ€§ (Attributes)
- `CAUSAL_LM`
- `SEQ_2_SEQ_LM`
- `SEQ_CLS`

---

## Class: `UsageInfo`

- **è¯­è¨€ (Language):** python
- **æ¨¡å—/æ–‡ä»¶ (Module/File):** `openai_api_protocol_vllm`

### ç»§æ‰¿å…³ç³» (Inheritance)
- ç»§æ‰¿è‡ª (Inherits from): `BaseModel`

### å±æ€§ (Attributes)
- `completion_tokens: Optional[int]`
- `prompt_tokens: int`
- `total_tokens: int`

---


[ä¿¡æ¯] UML å›¾è¡¨ 'llama2_project_uml.png' ä¹Ÿå·²é€šè¿‡ WebSocket ç›´æ¥å‘é€ã€‚
2025-05-15 00:09:31.258 | INFO     | agents.umlagent:update_plan_status:155 - Marked step 2 as completed in plan plan_1747238923
2025-05-15 00:09:31.259 | INFO     | agents.base:run:126 - Executing step 4/20
2025-05-15 00:09:44.971 | INFO     | agents.tool_call:think:56 - âœ¨ uml agnet çš„æƒ³æ³•ä¸º: Using tools response: å·²æˆåŠŸç”ŸæˆLlama2é¡¹ç›®çš„UMLå›¾ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘å°†æ‰§è¡Œç¬¬å››æ­¥ï¼šåˆ†æå¹¶æä¾›UMLå›¾çš„è§£é‡Šã€‚
2025-05-15 00:09:44.972 | INFO     | agents.tool_call:think:58 - ğŸ› ï¸ uml agnet é€‰æ‹©äº† 1 ä¸ªå·¥å…·
2025-05-15 00:09:44.972 | INFO     | agents.tool_call:think:63 - ğŸ§° é€‰æ‹©çš„å·¥å…·ä¿¡æ¯: ['final_response']
2025-05-15 00:09:44.972 | INFO     | agents.tool_call:think:78 - ğŸ§° å·¥å…·çš„å‚æ•°æ˜¯: ['{"content_to_deliver": "å·²æ‰¾åˆ°ä¸€ä¸ªåä¸º\'Chinese-LLaMA-Alpaca-2\'çš„Llama2é¡¹ç›®ï¼Œå¹¶ç”Ÿæˆäº†å…¶UMLå›¾ã€‚ä»¥ä¸‹æ˜¯UMLå›¾çš„è§£é‡Šï¼š\\n\\n1. ChatCompletionRequestç±»ï¼šç”¨äºè¡¨ç¤ºèŠå¤©å®Œæˆè¯·æ±‚ï¼ŒåŒ…å«å¤šä¸ªå±æ€§å¦‚messagesã€modelç­‰ã€‚\\n2. ChatCompletionResponseç±»ï¼šç”¨äºè¡¨ç¤ºèŠå¤©å®Œæˆå“åº”ï¼ŒåŒ…å«choicesã€createdç­‰å±æ€§ã€‚\\n3. ChineseLLaMAAlpaca2Adapterç±»ï¼šç»§æ‰¿è‡ªBaseModelAdapterï¼Œæä¾›äº†get_default_conv_templateå’Œload_modelç­‰æ–¹æ³•ã€‚\\n4. Llama_Evaluatorç±»ï¼šç»§æ‰¿è‡ªEvaluatorï¼Œæä¾›äº†eval_subjectã€extract_answerç­‰æ–¹æ³•ã€‚\\n5. PeftModelç±»ï¼šç»§æ‰¿è‡ªPushToHubMixinå’Œtorch.nn.Moduleï¼Œæä¾›äº†disable_adapterã€forwardç­‰æ–¹æ³•ã€‚\\n\\næ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒç”Ÿæˆçš„UMLå›¾ã€‚"}']
2025-05-15 00:09:44.973 | INFO     | tools.summarize:execute:35 - ğŸ ä»£ç†çš„æœ€ç»ˆå›åº”: å·²æ‰¾åˆ°ä¸€ä¸ªåä¸º'Chinese-LLaMA-Alpaca-2'çš„Llama2é¡¹ç›®ï¼Œå¹¶ç”Ÿæˆäº†å…¶UMLå›¾ã€‚ä»¥ä¸‹æ˜¯UMLå›¾çš„è§£é‡Šï¼š

1. ChatCompletionRequestç±»ï¼šç”¨äºè¡¨ç¤ºèŠå¤©å®Œæˆè¯·æ±‚ï¼ŒåŒ…å«å¤šä¸ªå±æ€§å¦‚messagesã€modelç­‰ã€‚
2. ChatCompletionResponseç±»ï¼šç”¨äºè¡¨ç¤ºèŠå¤©å®Œæˆå“åº”ï¼ŒåŒ…å«choicesã€createdç­‰å±æ€§ã€‚
3. ChineseLLaMAAlpaca2Adapterç±»ï¼šç»§æ‰¿è‡ªBaseModelAdapterï¼Œæä¾›äº†get_default_conv_templateå’Œload_modelç­‰æ–¹æ³•ã€‚
4. Llama_Evaluatorç±»ï¼šç»§æ‰¿è‡ªEvaluatorï¼Œæä¾›äº†eval_subjectã€extract_answerç­‰æ–¹æ³•ã€‚
5. PeftModelç±»ï¼šç»§æ‰¿è‡ªPushToHubMixinå’Œtorch.nn.Moduleï¼Œæä¾›äº†disable_adapterã€forwardç­‰æ–¹æ³•ã€‚

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒç”Ÿæˆçš„UMLå›¾ã€‚
2025-05-15 00:09:44.973 | INFO     | agents.tool_call:act:131 - ğŸ¯ å·¥å…· 'final_response' å®Œæˆäº†å®ƒçš„ä»»åŠ¡! å…¶æ‰§è¡Œç»“æœä¸º:  `å·¥å…·:final_response`çš„è§‚æµ‹ç»“æœè¾“å‡ºä¸º :
ä»£ç†å·²å¾—å‡ºä»¥ä¸‹æœ€ç»ˆå›åº”: å·²æ‰¾åˆ°ä¸€ä¸ªåä¸º'Chinese-LLaMA-Alpaca-2'çš„Llama2é¡¹ç›®ï¼Œå¹¶ç”Ÿæˆäº†å…¶UMLå›¾ã€‚ä»¥ä¸‹æ˜¯UMLå›¾çš„è§£é‡Šï¼š

1. ChatCompletionRequestç±»ï¼šç”¨äºè¡¨ç¤ºèŠå¤©å®Œæˆè¯·æ±‚ï¼ŒåŒ…å«å¤šä¸ªå±æ€§å¦‚messagesã€modelç­‰ã€‚
2. ChatCompletionResponseç±»ï¼šç”¨äºè¡¨ç¤ºèŠå¤©å®Œæˆå“åº”ï¼ŒåŒ…å«choicesã€createdç­‰å±æ€§ã€‚
3. ChineseLLaMAAlpaca2Adapterç±»ï¼šç»§æ‰¿è‡ªBaseModelAdapterï¼Œæä¾›äº†get_default_conv_templateå’Œload_modelç­‰æ–¹æ³•ã€‚
4. Llama_Evaluatorç±»ï¼šç»§æ‰¿è‡ªEvaluatorï¼Œæä¾›äº†eval_subjectã€extract_answerç­‰æ–¹æ³•ã€‚
5. PeftModelç±»ï¼šç»§æ‰¿è‡ªPushToHubMixinå’Œtorch.nn.Moduleï¼Œæä¾›äº†disable_adapterã€forwardç­‰æ–¹æ³•ã€‚

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒç”Ÿæˆçš„UMLå›¾ã€‚
2025-05-15 00:09:44.973 | INFO     | agents.umlagent:update_plan_status:155 - Marked step 3 as completed in plan plan_1747238923
2025-05-15 00:09:44.973 | INFO     | agents.base:run:126 - Executing step 5/20
2025-05-15 00:09:49.847 | INFO     | agents.tool_call:think:56 - âœ¨ uml agnet çš„æƒ³æ³•ä¸º: Using tools response: æ‰€æœ‰æ­¥éª¤å·²å®Œæˆï¼Œç°åœ¨å¯ä»¥ç»“æŸä»»åŠ¡ã€‚
2025-05-15 00:09:49.847 | INFO     | agents.tool_call:think:58 - ğŸ› ï¸ uml agnet é€‰æ‹©äº† 1 ä¸ªå·¥å…·
2025-05-15 00:09:49.848 | INFO     | agents.tool_call:think:63 - ğŸ§° é€‰æ‹©çš„å·¥å…·ä¿¡æ¯: ['terminate']
2025-05-15 00:09:49.848 | INFO     | agents.tool_call:think:78 - ğŸ§° å·¥å…·çš„å‚æ•°æ˜¯: ['{"status": "success"}']
2025-05-15 00:09:49.848 | INFO     | agents.tool_call:_handle_special_tool:183 - ğŸ Special tool 'terminate' has completed the task!
2025-05-15 00:09:49.848 | INFO     | agents.tool_call:act:131 - ğŸ¯ å·¥å…· 'terminate' å®Œæˆäº†å®ƒçš„ä»»åŠ¡! å…¶æ‰§è¡Œç»“æœä¸º:  `å·¥å…·:terminate`çš„è§‚æµ‹ç»“æœè¾“å‡ºä¸º :
The interaction has been completed with status: success
2025-05-15 00:11:12.731 | INFO     | main:websocket_endpoint:61 - WebSocket connection disconnected from 127.0.0.1:56017
2025-05-15 00:11:12.731 | INFO     | main:websocket_endpoint:76 - Cleaned up resources for 127.0.0.1:56017
2025-05-15 08:37:58.089 | INFO     | main:websocket_endpoint:36 - WebSocket connection accepted from 127.0.0.1:57100
2025-05-15 08:50:18.210 | INFO     | main:websocket_endpoint:61 - WebSocket connection disconnected from 127.0.0.1:57100
2025-05-15 08:50:18.212 | INFO     | main:websocket_endpoint:76 - Cleaned up resources for 127.0.0.1:57100
